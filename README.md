STEALGNN Attack Implementation

This repository implements and recreates the attacks described in the paper "Unveiling the Secrets without Data: Can Graph Neural Networks Be Exploited through Data-Free Model Extraction Attacks?", presented at the 33rd USENIX Security Symposium (2024). The paper proposes a novel framework called STEALGNN, which demonstrates the effectiveness of data-free model extraction attacks on Graph Neural Networks (GNNs) without access to actual graph data or node features.
Overview

Graph Neural Networks (GNNs) have become essential in applications such as social science, biology, and molecular chemistry. However, these models are vulnerable to model extraction attacks, where an adversary can extract the model's functionality by querying it and training a surrogate model. STEALGNN presents the first data-free extraction attack on GNNs, making it a significant contribution to the study of intellectual property threats to machine learning models.

This repository contains Python code that reproduces three types of attacks described in the STEALGNN paper, using synthetic graphs generated by a graph generator to train a surrogate model that mimics the behavior of a victim model.
Attack Types Implemented

    Type I Attack:
        Utilizes gradients propagated back through both a surrogate model and an estimated gradient approximation of the victim model.
        This attack allows the generator to optimize by leveraging knowledge from both sources.

    Type II Attack:
        Relies solely on the gradients from one surrogate model, simplifying the gradient flow but still enabling the generator to learn from the surrogate.

    Type III Attack:
        Involves two surrogate models. Discrepancies between their predictions allow the surrogate models to acquire more complex knowledge, improving the mimicry of the victim model.

Project Structure

    models/: Contains the core model code, including:
        __init__.py: Initialization file for the models.
        generator.py: Contains the graph generator model to generate synthetic graphs.
        surrogate.py: Implements the surrogate model.
        victim.py: Simulates the victim model.

    Root Directory:
        type1attack.py: Implements the Type I attack on the victim model.
        main.py: Entry point for running experiments.

How to Run

    Install Dependencies:
        Ensure you have Python 3.8 or higher installed.
        Install required packages with:

        bash

    pip install -r requirements.txt

Running the Attacks:

    You can run each attack type using the following command:

    bash

        python type1attack.py  # Currently Type I attack is implemented

    Experiments:
        You can adjust the code in main.py to run experiments with different configurations of victim and surrogate models.

Evaluation Metrics

    Accuracy: Measures how closely the surrogate model replicates the predictions of the victim model.
    Fidelity: Quantifies the agreement between the surrogate and victim model's predictions.
    Discrepancy Loss: Used in Type III attacks to enforce prediction differences between surrogate models for better learning.

References

    Zhuang, Y., Shi, C., Zhang, M., Chen, J., Lyu, L., Zhou, P., & Sun, L. (2024). Unveiling the Secrets without Data: Can Graph Neural Networks Be Exploited through Data-Free Model Extraction Attacks? USENIX Security Symposium, 2024.
